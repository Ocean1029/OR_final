#!/usr/bin/env python3
# aggregate_interval.py
#
# éœ€æ±‚ï¼š
#   pip install pandas pytz
# ç”¨æ³•ç¯„ä¾‹ï¼š
#   python aggregate_interval.py \
#       --data-dir data-log/data \
#       --output avg_by_interval.csv
#
#   ç”¢å‡ºæ¬„ä½ï¼š
#     interval_start  (Asia/Taipei, 30 åˆ†é˜ç²’åº¦)
#     sno             ç«™é»ä»£ç¢¼
#     sarea           è¡Œæ”¿å€
#     sna             ç«™å
#     lat, lon        åº§æ¨™
#     avg_total       å¹³å‡æ¨æ•¸
#     avg_empty       å¹³å‡ç©ºä½
#
# è³‡æ–™æ¬„ä½ä¾†æºè¦‹ fetch_snapshot.py ä¸­çš„å®šç¾© :contentReference[oaicite:0]{index=0}
# ---------------------------------------------------------------

from pathlib import Path
import argparse
import pandas as pd

TZ = "Asia/Taipei"          # ç›®æ¨™æ™‚å€ (GMT+8)

def load_snapshots(data_dir: Path) -> pd.DataFrame:
    """æŠŠè³‡æ–™å¤¾è£æ‰€æœ‰ snapshot_*.csv ä¸²æ¥èµ·ä¾†"""
    files = sorted(data_dir.glob("snapshot_*.csv"))
    if not files:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ä»»ä½• snapshot_*.csv æ–¼ {data_dir}")
    dfs = [pd.read_csv(f) for f in files]
    dfs = [df for df in dfs if not df.empty]  # ğŸš¨ éæ¿¾æ‰ç©ºçš„ DataFrame
    if not dfs:
        raise ValueError("æ‰€æœ‰ snapshot æª”æ¡ˆéƒ½ç‚ºç©ºï¼Œç„¡æ³•åˆä½µ")
    df = pd.concat(dfs, ignore_index=True)
    return df

def preprocess(df: pd.DataFrame) -> pd.DataFrame:
    """æ™‚æˆ³è½‰æ™‚å€ã€å°é½Š 30 åˆ†é˜å€æ®µã€å‹åˆ¥è½‰æ›"""
    
    df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce", utc=True)
    df["timestamp"] = df["timestamp"].dt.tz_convert(TZ)
    df["interval_start"] = df["timestamp"].dt.floor("30min")
    df["total"] = pd.to_numeric(df["total"], errors="coerce")
    df["available_return_bikes"] = pd.to_numeric(
        df["available_return_bikes"], errors="coerce"
    )
    return df

def aggregate(df: pd.DataFrame) -> dict:
    """ä¾ç…§æ¯å€‹ 30 åˆ†é˜æ™‚æ®µè·¨æ—¥å¹³å‡ï¼Œä¸¦åˆ†åˆ¥å›å‚³æ¯æ®µçš„ DataFrame"""
    df["time_only"] = df["interval_start"].dt.time  # åªå–æ™‚é–“éƒ¨åˆ†ï¼ˆä¾‹å¦‚ 08:30:00ï¼‰

    grouped = (
        df.groupby(["time_only", "sno"])
          .agg(
              sarea=("sarea", "first"),
              sna=("sna", "first"),
              lat=("latitude", "first"),
              lon=("longitude", "first"),
              avg_total=("total", "mean"),
              avg_empty=("available_return_bikes", "mean"),
          )
          .reset_index()
    )
    
    # æŒ‰ç…§æ¯å€‹æ™‚æ®µåˆ‡åˆ† DataFrameï¼Œå›å‚³ dict
    by_interval = {
        t.strftime("%H%M"): g.reset_index(drop=True)
        for t, g in grouped.groupby("time_only")
    }
    return by_interval

def main():
    parser = argparse.ArgumentParser(description="Ubike æ™‚æ®µå¹³å‡åˆ†æå™¨")
    parser.add_argument("--data-dir", default="data/", help="åŒ…å« snapshot_*.csv çš„è³‡æ–™å¤¾")
    parser.add_argument("--output-dir", default="interval_outputs", help="è¼¸å‡ºè³‡æ–™å¤¾")
    args = parser.parse_args()

    data_dir = Path(args.data_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    df_raw = load_snapshots(data_dir)
    df_pre = preprocess(df_raw)
    interval_dict = aggregate(df_pre)

    for time_str, df_interval in interval_dict.items():
        filename = output_dir / f"interval_{time_str}.csv"
        df_interval.to_csv(filename, index=False, encoding="utf-8")
        print(f"âœ” è¼¸å‡º {filename.name}ï¼ˆ{len(df_interval)} ç­†ï¼‰")

    print(f"\nğŸ‰ å…±è¼¸å‡º {len(interval_dict)} å€‹æ™‚æ®µæª”æ¡ˆåˆ° {output_dir}/")

if __name__ == "__main__":
    main()
